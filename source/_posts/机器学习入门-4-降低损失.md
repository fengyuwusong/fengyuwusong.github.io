---
title: 机器学习入门(4)-降低损失
date: 2018-12-19 11:46:12
categories:
- 机器学习
tags: [损失, 机器学习, 学习笔记, 迭代方法， 学习速率， 梯度下降]
mathjax: true
---

#### 问题引出

上一节有提到**损失**用于表示模型对于单个样本预测准确程度的一个数值，那么问题就转移到了我们该如何降低损失呢？

## 迭代方法

![mark](https://pic.fengyuwusong.cn/MPic/20181219/53LXBTUXHElx.png)

**图 1. 用于训练模型的迭代方法。**

<!-- more -->

在机器学习中，迭代方法的应用十分的普遍，他的逻辑是通过不停的尝试来降低损失值。

例如对于线性回归公式
$$
y'=b+w_1x_1
$$
**b **和 **w1**的初始值应该设置为什么比较好呢？

事实证明初始值并不重要。我们可以随机选择值，不过我们还是选择采用以下这些无关紧要的值：

- b= 0
- w1 = 0

假设第一个特征值是 10。将该特征值代入预测函数会得到以下结果：

```
  y' = 0 + 0(10)
  y' = 0
```

图中的“计算损失”部分是模型将要使用的[损失函数](https://developers.google.cn/machine-learning/crash-course/descending-into-ml/training-and-loss)。假设我们使用平方损失函数。损失函数将采用两个输入值：

- y'：模型对特征 x 的预测
- y：特征 x 对应的正确标签。

这时需要进行的是图一中的 **计算参数更新**， 具体是计算该 b 和 w1 下的损失值，然后再生成新的 b 和 w1。 这个过程会持续迭代，直至算法发现损失可能是最低的模型参数。

**收敛**： 当在不断迭代的情况下，损失值不变或十分缓慢，这时可以说该模型已经收敛。



## 该怎么进行计算参数？

在上面的迭代方法中，最重要的一步就是计算参数更新。那么该怎么计算参数直至模型收敛呢？

对于回归问题而言，所产生的损失和 **w1** 的关系是个凸形的，即只有一个斜率为0的位置：

![mark](https://pic.fengyuwusong.cn/MPic/20181219/73H0L044vBuC.png) 

**图 2. 回归问题产生的损失与权重图为凸形。**

该斜率为 0 的地方即损失函数的收敛之处。

### 梯度下降法

梯度下降法的第一个阶段是为 w1 选择一个起始值（起点）。起点并不重要；因此很多算法就直接将 w1 设为 0 或随机选择一个值。下图显示的是我们选择了一个稍大于 0 的起点：

![mark](https://pic.fengyuwusong.cn/MPic/20181219/F7JPHnUY4IuD.png)

**图 3. 梯度下降法的起点。**

然后，梯度下降法算法会计算损失曲线在起点处的梯度。简而言之，**梯度**是偏导数的矢量；它可以让您了解哪个方向距离目标“更近”或“更远”。请注意，损失相对于单个权重的梯度（如图 3 所示）就等于导数。

主要的数学理论之后再写博客介绍，此处涉及的有 ***导数、偏导数、梯度***。



请注意，梯度是一个矢量，因此具有以下两个特征：

- 方向
- 大小

梯度始终指向损失函数中增长最为迅猛的方向。梯度下降法算法会沿着负梯度的方向走一步，以便尽快降低损失。

![mark](https://pic.fengyuwusong.cn/MPic/20181219/vTnBdkhvlMir.png)

**图 4. 梯度下降法依赖于负梯度。**

为了确定损失函数曲线上的下一个点，梯度下降法算法会将梯度大小的一部分与起点相加，如下图所示：



![1545230013329](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1545230013329.png)

**图 5. 一个梯度步长将我们移动到损失曲线上的下一个点。**

然后，梯度下降法会重复此过程，逐渐接近最低点。



## 学习速率

**学习速率**： 决定迭代中下一个点的位置， 一般是梯度乘以学习速率。

**超参数**：编程人员在机器学习算法中用于调整的旋钮

学习速率不宜过大或过小。

![mark](https://pic.fengyuwusong.cn/MPic/20181219/pq7kOK0KzpPp.png)

## 随机梯度下降法

**批量**： 用于在单次迭代中计算梯度的样本总数。

**随机梯度下降法** (**SGD**) ： 每次只迭代计算一个随机样本的损失值，减少计算量。

**小批量随机梯度下降法**（**小批量 SGD**）： 每次只迭代计算小批量随机样本的损失值，小批量通常包含 10-1000 个随机选择的样本。

