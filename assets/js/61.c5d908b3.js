(window.webpackJsonp=window.webpackJsonp||[]).push([[61],{401:function(v,t,_){"use strict";_.r(t);var s=_(0),r=Object(s.a)({},(function(){var v=this,t=v._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":v.$parent.slotKey}},[t("h4",{attrs:{id:"问题引出"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#问题引出"}},[v._v("#")]),v._v(" 问题引出")]),v._v(" "),t("p",[v._v("上一节有提到"),t("strong",[v._v("损失")]),v._v("用于表示模型对于单个样本预测准确程度的一个数值，那么问题就转移到了我们该如何降低损失呢？")]),v._v(" "),t("h2",{attrs:{id:"迭代方法"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#迭代方法"}},[v._v("#")]),v._v(" 迭代方法")]),v._v(" "),t("p",[t("img",{attrs:{src:"https://pic.fengyuwusong.cn/MPic/20181219/53LXBTUXHElx.png",alt:"mark"}})]),v._v(" "),t("p",[t("strong",[v._v("图 1. 用于训练模型的迭代方法。")])]),v._v(" "),t("p",[v._v("在机器学习中，迭代方法的应用十分的普遍，他的逻辑是通过不停的尝试来降低损失值。")]),v._v(" "),t("p",[v._v("例如对于线性回归公式\n$$\ny'=b+w_1x_1\n$$\n**b **和 "),t("strong",[v._v("w1")]),v._v("的初始值应该设置为什么比较好呢？")]),v._v(" "),t("p",[v._v("事实证明初始值并不重要。我们可以随机选择值，不过我们还是选择采用以下这些无关紧要的值：")]),v._v(" "),t("ul",[t("li",[v._v("b= 0")]),v._v(" "),t("li",[v._v("w1 = 0")])]),v._v(" "),t("p",[v._v("假设第一个特征值是 10。将该特征值代入预测函数会得到以下结果：")]),v._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[v._v("  y' = 0 + 0(10)\n  y' = 0\n")])]),v._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[v._v("1")]),t("br"),t("span",{staticClass:"line-number"},[v._v("2")]),t("br")])]),t("p",[v._v("图中的“计算损失”部分是模型将要使用的"),t("a",{attrs:{href:"https://developers.google.cn/machine-learning/crash-course/descending-into-ml/training-and-loss",target:"_blank",rel:"noopener noreferrer"}},[v._v("损失函数"),t("OutboundLink")],1),v._v("。假设我们使用平方损失函数。损失函数将采用两个输入值：")]),v._v(" "),t("ul",[t("li",[v._v("y'：模型对特征 x 的预测")]),v._v(" "),t("li",[v._v("y：特征 x 对应的正确标签。")])]),v._v(" "),t("p",[v._v("这时需要进行的是图一中的 "),t("strong",[v._v("计算参数更新")]),v._v("， 具体是计算该 b 和 w1 下的损失值，然后再生成新的 b 和 w1。 这个过程会持续迭代，直至算法发现损失可能是最低的模型参数。")]),v._v(" "),t("p",[t("strong",[v._v("收敛")]),v._v("： 当在不断迭代的情况下，损失值不变或十分缓慢，这时可以说该模型已经收敛。")]),v._v(" "),t("h2",{attrs:{id:"该怎么进行计算参数"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#该怎么进行计算参数"}},[v._v("#")]),v._v(" 该怎么进行计算参数？")]),v._v(" "),t("p",[v._v("在上面的迭代方法中，最重要的一步就是计算参数更新。那么该怎么计算参数直至模型收敛呢？")]),v._v(" "),t("p",[v._v("对于回归问题而言，所产生的损失和 "),t("strong",[v._v("w1")]),v._v(" 的关系是个凸形的，即只有一个斜率为0的位置：")]),v._v(" "),t("p",[t("img",{attrs:{src:"https://pic.fengyuwusong.cn/MPic/20181219/73H0L044vBuC.png",alt:"mark"}})]),v._v(" "),t("p",[t("strong",[v._v("图 2. 回归问题产生的损失与权重图为凸形。")])]),v._v(" "),t("p",[v._v("该斜率为 0 的地方即损失函数的收敛之处。")]),v._v(" "),t("h3",{attrs:{id:"梯度下降法"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#梯度下降法"}},[v._v("#")]),v._v(" 梯度下降法")]),v._v(" "),t("p",[v._v("梯度下降法的第一个阶段是为 w1 选择一个起始值（起点）。起点并不重要；因此很多算法就直接将 w1 设为 0 或随机选择一个值。下图显示的是我们选择了一个稍大于 0 的起点：")]),v._v(" "),t("p",[t("img",{attrs:{src:"https://pic.fengyuwusong.cn/MPic/20181219/F7JPHnUY4IuD.png",alt:"mark"}})]),v._v(" "),t("p",[t("strong",[v._v("图 3. 梯度下降法的起点。")])]),v._v(" "),t("p",[v._v("然后，梯度下降法算法会计算损失曲线在起点处的梯度。简而言之，"),t("strong",[v._v("梯度")]),v._v("是偏导数的矢量；它可以让您了解哪个方向距离目标“更近”或“更远”。请注意，损失相对于单个权重的梯度（如图 3 所示）就等于导数。")]),v._v(" "),t("p",[v._v("主要的数学理论之后再写博客介绍，此处涉及的有 "),t("em",[t("strong",[v._v("导数、偏导数、梯度")])]),v._v("。")]),v._v(" "),t("p",[v._v("请注意，梯度是一个矢量，因此具有以下两个特征：")]),v._v(" "),t("ul",[t("li",[v._v("方向")]),v._v(" "),t("li",[v._v("大小")])]),v._v(" "),t("p",[v._v("梯度始终指向损失函数中增长最为迅猛的方向。梯度下降法算法会沿着负梯度的方向走一步，以便尽快降低损失。")]),v._v(" "),t("p",[t("img",{attrs:{src:"https://pic.fengyuwusong.cn/MPic/20181219/vTnBdkhvlMir.png",alt:"mark"}})]),v._v(" "),t("p",[t("strong",[v._v("图 4. 梯度下降法依赖于负梯度。")])]),v._v(" "),t("p",[v._v("为了确定损失函数曲线上的下一个点，梯度下降法算法会将梯度大小的一部分与起点相加，如下图所示：")]),v._v(" "),t("p",[t("img",{attrs:{src:"C:%5CUsers%5CAdministrator%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1545230013329.png",alt:"1545230013329"}})]),v._v(" "),t("p",[t("strong",[v._v("图 5. 一个梯度步长将我们移动到损失曲线上的下一个点。")])]),v._v(" "),t("p",[v._v("然后，梯度下降法会重复此过程，逐渐接近最低点。")]),v._v(" "),t("h2",{attrs:{id:"学习速率"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#学习速率"}},[v._v("#")]),v._v(" 学习速率")]),v._v(" "),t("p",[t("strong",[v._v("学习速率")]),v._v("： 决定迭代中下一个点的位置， 一般是梯度乘以学习速率。")]),v._v(" "),t("p",[t("strong",[v._v("超参数")]),v._v("：编程人员在机器学习算法中用于调整的旋钮")]),v._v(" "),t("p",[v._v("学习速率不宜过大或过小。")]),v._v(" "),t("p",[t("img",{attrs:{src:"https://pic.fengyuwusong.cn/MPic/20181219/pq7kOK0KzpPp.png",alt:"mark"}})]),v._v(" "),t("h2",{attrs:{id:"随机梯度下降法"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#随机梯度下降法"}},[v._v("#")]),v._v(" 随机梯度下降法")]),v._v(" "),t("p",[t("strong",[v._v("批量")]),v._v("： 用于在单次迭代中计算梯度的样本总数。")]),v._v(" "),t("p",[t("strong",[v._v("随机梯度下降法")]),v._v(" ("),t("strong",[v._v("SGD")]),v._v(") ： 每次只迭代计算一个随机样本的损失值，减少计算量。")]),v._v(" "),t("p",[t("strong",[v._v("小批量随机梯度下降法")]),v._v("（"),t("strong",[v._v("小批量 SGD")]),v._v("）： 每次只迭代计算小批量随机样本的损失值，小批量通常包含 10-1000 个随机选择的样本。")])])}),[],!1,null,null,null);t.default=r.exports}}]);